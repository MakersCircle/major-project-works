{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:47.400052Z",
     "start_time": "2024-12-15T20:15:45.590522Z"
    }
   },
   "source": [
    "import re\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision.models.video import swin3d_t\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, ToPILImage"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:47.410269Z",
     "start_time": "2024-12-15T20:15:47.403950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, annotation_file, video_dir, sequence_length=32, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation_file (str): Path to the annotations file (e.g., Crash-1500.txt).\n",
    "            video_dir (str): Directory containing the video files.\n",
    "            sequence_length (int): Number of frames in each input sequence.\n",
    "            transform (callable, optional): A function/transform to apply to video frames.\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load and parse annotations\n",
    "        self.data = []\n",
    "\n",
    "        self.to_pil = ToPILImage()\n",
    "\n",
    "\n",
    "        with open(annotation_file) as f:\n",
    "            for line in f.readlines():\n",
    "                # Use regex to extract fields properly\n",
    "                match = re.match(r\"^(\\d+),(\\[.*?\\]),(\\d+),(\\d+),(Day|Night),(Normal|Snowy|Rainy),(Yes|No)$\", line.strip())\n",
    "                if match:\n",
    "                    vidname = match.group(1)\n",
    "                    binlabels = eval(match.group(2))  # Safely evaluate the binary labels\n",
    "                    startframe = int(match.group(3))\n",
    "                    youtubeID = match.group(4)\n",
    "                    timing = match.group(5)\n",
    "                    weather = match.group(6)\n",
    "                    egoinvolve = match.group(7)\n",
    "\n",
    "                    self.data.append({\n",
    "                        'vidname': vidname,\n",
    "                        'binlabels': binlabels,\n",
    "                        'startframe': startframe,\n",
    "                        'youtubeID': youtubeID,\n",
    "                        'timing': timing,\n",
    "                        'weather': weather,\n",
    "                        'egoinvolve': egoinvolve\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        vidname = sample['vidname']\n",
    "        binlabels = sample['binlabels']\n",
    "\n",
    "        # Load video frames\n",
    "        video_path = video_dir / f'{vidname}.mp4'\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        success, frame = cap.read()\n",
    "        while success:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            success, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        # Ensure we have enough frames\n",
    "        if len(frames) < self.sequence_length:\n",
    "            raise ValueError(f\"Video {vidname} has fewer than {self.sequence_length} frames.\")\n",
    "\n",
    "        # Select the last `sequence_length` frames and labels\n",
    "        frames = frames[-self.sequence_length:]\n",
    "        labels = binlabels[-self.sequence_length:]\n",
    "\n",
    "        frames = [self.to_pil(frame) for frame in frames]\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "        frames = torch.stack(frames)  # Convert list of tensors to a 4D tensor\n",
    "        frames = frames.permute(1, 0, 2, 3)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return frames, labels"
   ],
   "id": "429df9bc529ab508",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:47.459397Z",
     "start_time": "2024-12-15T20:15:47.456762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "b38b76607a830acb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:49.439601Z",
     "start_time": "2024-12-15T20:15:49.437148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = Path().resolve().parent.parent / 'datasets' / 'ccd'\n",
    "annotation_file = dataset_path / 'Crash-1500.txt'\n",
    "video_dir = dataset_path / 'Crash-1500'\n",
    "sequence_length = 50"
   ],
   "id": "97457c38ad893257",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:53.138432Z",
     "start_time": "2024-12-15T20:15:53.079947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the dataset\n",
    "accident_dataset = AccidentDataset(annotation_file=annotation_file,\n",
    "                                   video_dir=video_dir,\n",
    "                                   sequence_length=sequence_length,\n",
    "                                   transform=transform)"
   ],
   "id": "488ce1f64b824548",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:53.601273Z",
     "start_time": "2024-12-15T20:15:53.598519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the dataloader\n",
    "dataloader = DataLoader(accident_dataset, batch_size=8, shuffle=True, num_workers=4)"
   ],
   "id": "d8d2849c928a6ae9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:13:55.307059Z",
     "start_time": "2024-12-15T20:13:46.762143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "for batch_frames, batch_labels in dataloader:\n",
    "    print(\"Batch frames shape:\", batch_frames.shape)  # Expected: [batch_size, 32, 3, 224, 224]\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)  # Expected: [batch_size, 32]\n",
    "    break\n"
   ],
   "id": "3e96a0b956fb5e6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch frames shape: torch.Size([8, 3, 50, 224, 224])\n",
      "Batch labels shape: torch.Size([8, 50])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "afa4fb7a4fc2e68e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:56.844441Z",
     "start_time": "2024-12-15T20:15:56.426911Z"
    }
   },
   "cell_type": "code",
   "source": "model = swin3d_t(weights=\"KINETICS400_V1\")",
   "id": "3ea38201d9e8eb68",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:57.415435Z",
     "start_time": "2024-12-15T20:15:57.412156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_features = model.head.in_features\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(num_features, 1),  # Predict probability of accident per frame\n",
    "    nn.Sigmoid()  # Output probabilities in [0, 1]\n",
    ")"
   ],
   "id": "bc2462918636bac5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:58.060715Z",
     "start_time": "2024-12-15T20:15:58.029549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ],
   "id": "b9d9cdb8e6234ae3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:59.038820Z",
     "start_time": "2024-12-15T20:15:59.035353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for each frame\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ],
   "id": "9aa3f0de39a6980a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T20:15:59.804350Z",
     "start_time": "2024-12-15T20:15:59.802012Z"
    }
   },
   "cell_type": "code",
   "source": "epochs = 1",
   "id": "1140ca67d84b2eb6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-15T20:16:00.830350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate through batches\n",
    "    for frames, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "        # Expand labels to match the model output shape\n",
    "        labels = labels.unsqueeze(2)  # Shape: [batch_size, sequence_length, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(frames)  # Shape: [batch_size, sequence_length, 1]\n",
    "        outputs = outputs.squeeze(2)  # Shape: [batch_size, sequence_length]\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Adjust learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
   ],
   "id": "1da3a173b98685da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/188 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), \"swin3d_finetuned.pth\")",
   "id": "685fdb4f7c4ccf2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
